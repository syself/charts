common: 
  # -- Defines labels which are shared between all created resources.
  # @default -- the chart will add some internal labels
  labels:
    standard: {}
cluster:
  # -- Specifies some custom annotations
  annotations: {}
  # -- Specifies some custom labels
  labels: {}
  # -- Tells the cluster-api controller to pause the cluster
  paused:
  # -- Specifies the IP Range for the Kubernetes cluster.
  network:
    # -- Specifies the podCIDR for Kubernetes
    podCIDRBlocks: []
    # @default -- the default serviceCIDR provided by the kubernetes installation tool (e.g Kubeadm)
    serviceCIDRBlocks: []
  # -- Specifies the internal Kubernetes Domain
  serviceDomain:
  # -- Specifies which Kubernetes installation method is used. Supported values: kubeadm
  controlPlaneRef: kubeadm
  # -- Specifies the infrastructure Provider. The corresponding controller needs to be installed before. See clusterctl init --infrastructure <infra-provider>.  Supported values: hetzner
  infrastructureRef: 
  # -- Configures the state of the control Plane. Will configure the underlying CR of KCP or similar and a optional HealthCheck for the controlPlane
  controlPlane: {}
    # # -- Sets the amount of created control Planes. For HA set replicas to 3 or 5.
    # replicas: 1
    # # -- Specifies the Kubernetes Version. This needs to match with the installed version of the node image usually specified in the machine Template of the infrastructure provider. If you update Kubernetes this needs to be updated first
    # version: 
    # # -- Specifies a sub resource of the infrastructure provider. For example if a infrastructure provider have more than one CR for specifing a machine Template (e.g Baremetal, cloud). For hetzner: hcloud
    # type: 
    # # -- Specifies a health Check for the control Planes
    # healthCheck: {}
      # enabled: false
      # # -- Specifies some custom labels for the HealthCheck CR
      # labels: {}
      # # -- Specifies some custom annotations for the HealthCheck CR
      # annotations: {}
      # Specifies how many controlPlanes could be unhealthy
      # maxUnhealthy: 50%
      # nodeStartupTimeout: 4m
      # unhealthyConditions:
      # - type: Ready
      #   status: Unknown
      #   timeout: 300s
      # - type: Ready
      #   status: "False"
      #   timeout: 300s
  # Configures the state of the worker nodes. Will configure the underlying MachineDeployment CR and a optional related HealthCheck  
  workers: {}
    # Specifies the name of the underlying machineDeployment. This needs also to be set equally on the Bootstrap Provider (e.g kubeadm) and on the infrastructure Provider (e.g hetzner)
    # md-0:
    #   # -- Enables or disables the machineDeployment
    #   enabled: false
    #   # -- Specifies the amount of nodes
    #   replicas:
    #   # -- Specifies the Kubernetes Version of this machineDeployment. This needs to match with the installed version of the node image usually specified in the machine Template of the infrastructure provider
    #   version:
    #   # -- Specifies a sub resource of the infrastructure provider. For example if a infrastructure provider have more than one CR for specifing a machine Template (e.g Baremetal, cloud). For hetzner: hcloud
    #   type: 
    #   # -- Specifies some custom labels for the MachineDeployment CR
    #   labels: {} 
    #   # -- Specifies some custom annotations for the MachineDeployment CR
    #   annotations: {}
    #   # -- Specifies the failureDomain for the machineDeployment. Colloquially known as the location of the nodes. Values are defined by the supported infrastructure provider
    #   failureDomain:
    #   # -- Configure the update and replacement strategy
    #   strategy:
    #     rollingUpdate:
    #       deletePolicy: 
    #       maxSurge: 1
    #       maxUnavailable: 0
    #   # -- The maximum time a drain could take from the cluster-api perspective. The real drain by kubernetes needs to be specified accordingly.
    #   nodeDrainTimeout: 20m
    #   revisionHistoryLimit: 
    #   progressDeadlineSeconds:
    #   # -- HealthCheck for the specified machineDeployment
    #   healthCheck:
    #     enabled: true
    #     labels: 
    #       test: labels
    #     annotations:
    #       annotation: test 
    #     maxUnhealthy: 50%
    #     nodeStartupTimeout: 4m
    #     unhealthyConditions:
    #     - type: Ready
    #       status: Unknown
    #       timeout: 300s
    #     - type: Ready
    #       status: "False"
    #       timeout: 300s

# The specified ControlPlaneRef from cluster.controlPlaneRef
kubeadm:
  # The configuration for the control Plane. Will configure the underlying KubeadmControlPlane CR
  controlPlane: {}
    # # -- Specifies some custom labels for the KCP CR
    # labels: {}
    # # -- Specifies some custom annotations for the KCP CR
    # annotations: {}
    # # -- The maximum time a drain could take from the cluster-api perspective. The real drain by kubernetes needs to be specified accordingly.
    # nodeDrainTimeout:
    # upgradeAfter:
    # # -- Configure the kube-apiserver
    # apiServer:
    #   # -- Configure the kube-apiserver flags
    #   extraArgs: {}
    #   # -- Configure extraVolumes
    #   extraVolumes: []
    #   # -- Specifies the timeout for the controlPlanes
    #   timeoutForControlPlane: 5m
    # # -- Configure the controller-manager
    # controllerManager:
    #   # -- Configure the controller-manager flags
    #   extraArgs: {}
    #   # -- Configure extraVolumes
    #   extraVolumes: []
    # # -- Configure the installed DNS solution 
    # dns:
    #   imageRepository: quay.io
    #   imageTag: coredns
    # # -- Configure etcd
    # etcd:
    #   local:
    #     dataDir: /var/lib/etcd
    #     # -- Configure the etcd flags
    #     extraArgs: {}
    # # -- Configure the scheduler
    # scheduler:
    #   extraArgs: {}
    #   # -- Configure extraVolumes
    #   extraVolumes: {}
    # # -- Configure files
    # files: []
    #   # - path: 
    #   #   owner: 
    #   #   permissions:
    #   #   content: 
    # # -- Configure Users on the node
    # users: []
    #   # - name: test
    #   #   groups: users
    #   #   sudo: "ALL=(ALL:ALL) ALL"
    #   #   shell: /bin/bash
    #   #   sshAuthorizedKeys:
    #   #     - test
    # # -- Configure initConfiguration for the control-plane
    # initConfiguration:
    #   criSocket: 
    #   taints:
    #   kubelet:
    #     # -- Configure kubelet flags for the initConfiguration
    #     extraArgs: {}
    # # -- Configure joinConfiguration for the control-plane 
    # joinConfiguration:
    #   criSocket: 
    #   taints:
    #   kubelet:
    #     # -- Configure kubelet flags for the joinConfiguration
    #     extraArgs: {}
    # # -- Set preKubeadmCommands which let you run commands before kubeadm init runs.
    # preKubeadmCommands: [] 
    # # -- Set postKubeadmCommands which let you run commands after kubeadm init runs.
    # postKubeadmCommands: []
    # # -- If true this adds a script for retrying joining.
    # useExperimentalRetryJoin: 
    # # -- Sets the verbosity level
    # verbosity:

  workers: {}
    # md-0:
    #   # -- Specifies some custom labels for the KubeadmConfigTemplate CR
    #   labels: {}
    #   # -- Specifies some custom annotations for the KubeadmConfigTemplate CR
    #   annotations: {}
    #   # -- Configure Files
    #   files: []
    #     # - path:
    #     #   owner: 
    #     #   permissions: 
    #     #   content: 
    #   users:
    #     # - name: test
    #     #   groups: users
    #     #   sudo: "ALL=(ALL:ALL) ALL"
    #     #   shell: /bin/bash
    #     #   sshAuthorizedKeys:
    #     #     - test
    #   # -- Configure joinConfiguration for the worker
    #   joinConfiguration:
    #     criSocket: 
    #     taints:
    #     kubelet:
    #       # -- Configure kubelet flags for the joinConfiguration
    #       extraArgs: {}
    #   # -- Set preKubeadmCommands which let you run commands before kubeadm init runs.
    #   preKubeadmCommands: [] 
    #   # -- Set postKubeadmCommands which let you run commands after kubeadm init runs.
    #   postKubeadmCommands: []
    #   # -- If true this adds a script for retrying joining.
    #   useExperimentalRetryJoin: 
    #   # -- Sets the verbosity level
    #   verbosity:
            
hetzner: {}
  # cluster:
  #   # -- Specifies some custom labels for the HetznerCluster CR
  #   labels: {}
  #   # -- Specifies some custom annotations for the HetznerCluster CR
  #   annotations: {}
  #   # -- Sets a private network in Hetzner Cloud
  #   network: {}
  #     # enabled: true
  #     # cidrBlock: 10.0.0.0/8
  #     # subnetCidrBlock: 10.0.0.0/24
  #     # networkZone: eu-central
  #   # -- Sets the controlPlaneRegion for the control-planes. 
  #   # @default -- Could be set to "fsn1", "ngb1", or "hel1"
  #   controlPlaneRegions: []
  #   controlPlaneEndpoint: 
  #     # -- Specifies the domain where the cluster is reachable. If unset the IP Adress of the loadbalancer is used. Setting a value is recommend for production use.
  #     # dns: 
  #     # -- Specifies the access port of the cluster. Configures the listen Port of the Loadbalancer.
  #     port: 443
  #     # -- 
  #   controlPlaneLoadBalancer:
  #     # -- Specifies the region where the loadbalancer should be created
  #     region: 
  #     # -- Specifies the loadbalancer type
  #     type: lb11
  #     # -- Specifies the algorithm type of the loadbalancer.
  #     algorithm: least_connection
  #     # -- Specifies the destination Port of the loadbalancer this must be equal with cluster.apiServerPort
  #     port: 6443
  #     # -- Here extra Targets for the Hetzner Loadbalancer could be specified. 
  #     extraServices: []
  #     # - listenPort: 443
  #     #   destinationPort: 6443
  #     #   protocol: tcp
  #   # Configures the cluster-wide SSH Key.
  #   sshKeys: []
  #   # - name: "test-key"
  #   hetznerSecretRef:
  #     # -- This is the name of the created secret
  #     name: hetzner
  #     key:
  #       # -- Specifies the name of the key in the created secret
  #       hcloudToken: token
  #   # -- Configure placement Groups for hetzner 
  #   placementGroups: []
  # hcloud:
  #   controlPlane:
  #     labels: 
  #       test: label
  #     annotations:
  #       test: annotation
  #     type: cp31
  #     imageName: fedora-34
  #     placementGroupName: control-plane
  #     sshKeys:
  #     - name: test-2
  #   workers:
  #     md-0:
  #       labels: 
  #         test: label
  #       annotations: 
  #         annotation: test
  #       type: cpx41
  #       imageName: test-image
  #       placementGroupName: md-0
  #       sshKeys: 
  #         - name: test-2